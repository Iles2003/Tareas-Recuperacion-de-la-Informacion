# -*- coding: utf-8 -*-
"""Saldaña Stiven Tarea: Embeddings.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MnNoU_ostJG6X5Cj1DF3a4iq3PfL9nY1

# Stiven Saldaña

# Ejercicio 6: Introducción a Dense Retrieval
## Objetivo de la práctica
Generar embeddings con sentence-transformers (SBERT, E5), y recuperarlos

## Parte 0: Carga del Corpus
## Actividad
1. Carga el corpus 20 Newsgroups desde sklearn.datasets.fetch_20newsgroups.
2. Limita el corpus a los primeros 2000 documentos para facilitar el procesamiento
"""

# se instala la libreria para usar los transformers
!pip install sentence-transformers scikit-learn numpy

# se importan librerias necesarias
import numpy as np
from sklearn.datasets import fetch_20newsgroups
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer

# Se puede escoger entre usar el modelo SEBERT O E5
# unicamente se descomenta y comenta la opcion que se
# desee usar
model_name = 'all-MiniLM-L6-v2'
# model_name = 'intfloat/e5-base'
model = SentenceTransformer(model_name)

# Se carga el dataset excepto los encabezados
newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))
docs_full = newsgroups.data
# Se cargan los primeros 2000 documentos
docs = docs_full[:2000]

docs

"""## Parte 2: Generación de Embeddings
## Actividad
1. Usa dos modelos de sentence-transformers. Puedes usar: 'all-MiniLM-L6-v2' (SBERT), o 'intfloat/e5-base' (E5). Cuando uses E5, antepon "passage: " a cada documento antes de codificar.
2. Genera los vectores de embeddings para todos los documentos usando el modelo seleccionado.
3. Guarda los embeddings en un array de NumPy para su posterior indexación.
"""

# condicional en caso de esoger SEBERT o E5
texts_to_encode = docs
if "e5" in model_name:
    texts_to_encode = ["passage: " + doc for doc in docs]

# generacion de los vectores
doc_embeddings = model.encode(texts_to_encode, show_progress_bar=True)

# se guarda los embedings en un array  NumPy
doc_embeddings_np = np.array(doc_embeddings)
print(f"--> Embeddings generados. Forma del array: {doc_embeddings_np.shape}")

"""## Parte 3: Consulta
## Actividad
1. Escribe una consulta en lenguaje natural. Ejemplos:
* "God, religion, and spirituality"
* "space exploration"
* "car maintenance"
2. Codifica la consulta utilizando el mismo modelo de embeddings. Cuando uses E5, antepon "query: " a la consulta.

3. Recupera los 5 documentos más relevantes con similitud coseno.

4. Muestra los textos de los documentos recuperados (puedes mostrar solo los primeros 500 caracteres de cada uno).
"""

# escritura de la consulta
query_text = "space exploration"

# logica que se usa en caso de usar E5
query_input = query_text
if "e5" in model_name:
    query_input = "query: " + query_text

# se codifica la consulta
query_embedding = model.encode([query_input])

# se calcula la similitud coseno entre la consulta y los documentos
# se debuelve una matriz y toma la primera fila
similarities = cosine_similarity(query_embedding, doc_embeddings_np)[0]

# se toma los los 5 documentos
top_k = 5
top_results_indices = np.argsort(similarities)[::-1][:top_k]

print(f"\nResultados para: SEBERT con la consulta: '{query_text}'\n" + "="*40)

for rank, index in enumerate(top_results_indices):
    score = similarities[index]
    content = docs[index]

    print(f"\nTop {rank+1} (Score: {score:.4f})")
    print("-" * 20)
    # Mostramos solo los primeros 500 caracteres
    print(content[:500].strip())

"""# USANDO E5

## Selección del modelo
"""

# Se puede escoger entre usar el modelo SEBERT O E5
# unicamente se descomenta y comenta la opcion que se desee usar
# model_name = 'all-MiniLM-L6-v2'
model_name = 'intfloat/e5-base'
model = SentenceTransformer(model_name)

"""# Paso 0"""

# Se carga el dataset excepto los encabezados
newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))
docs_full = newsgroups.data
# Se cargan los primeros 2000 documentos
docs = docs_full[:2000]

docs

"""# Paso 2"""

# condicional en caso de esoger SEBERT o E5
texts_to_encode = docs
if "e5" in model_name:
    texts_to_encode = ["passage: " + doc for doc in docs]

# generacion de los vectores
doc_embeddings = model.encode(texts_to_encode, show_progress_bar=True)

# se guarda los embedings en un array  NumPy
doc_embeddings_np = np.array(doc_embeddings)
print(f"--> Embeddings generados. Forma del array: {doc_embeddings_np.shape}")

"""# Paso 3"""

# escritura de la consulta
query_text2 = "space exploration"

# logica que se usa en caso de usar E5
query_input2 = query_text2
if "e5" in model_name:
    query_input2 = "query: " + query_text2

# se codifica la consulta
query_embedding = model.encode([query_input2])

# se calcula la similitud coseno entre la consulta y los documentos
similarities = cosine_similarity(query_embedding, doc_embeddings_np)[0]

# se toma los 5 documentos
top_k = 5
top_results_indices = np.argsort(similarities)[::-1][:top_k]

"""# Reaultados"""

print(f"\nResultados para E5: '{query_text2}'\n" + "="*40)

for rank, index in enumerate(top_results_indices):
    score2 = similarities[index]
    content2 = docs[index]

    print(f"\nTop {rank+1} (Score: {score2:.4f})")
    print("-" * 20)
    print(content2[:500].strip())

import json

# Cambia esto por el nombre exacto del archivo que subiste
nombre_archivo_entrada = 'Saldaña_Stiven_Tarea_Embeddings.ipynb'
nombre_archivo_salida = 'notebook_fixed.ipynb'

# Leer el archivo
try:
    with open(nombre_archivo_entrada, 'r', encoding='utf-8') as f:
        notebook = json.load(f)

    # Buscar y eliminar la clave problemática
    if 'metadata' in notebook and 'widgets' in notebook['metadata']:
        del notebook['metadata']['widgets']
        print("¡Éxito! Se encontró y eliminó la sección 'metadata.widgets'.")
    else:
        print("No se encontró 'metadata.widgets'. El archivo podría estar limpio o el error es otro.")

    # Guardar el archivo arreglado
    with open(nombre_archivo_salida, 'w', encoding='utf-8') as f:
        json.dump(notebook, f, indent=1)

    print(f"Descarga el archivo '{nombre_archivo_salida}' y súbelo al repositorio.")

except FileNotFoundError:
    print(f"Error: No se encontró el archivo '{nombre_archivo_entrada}'. Verifica el nombre.")

