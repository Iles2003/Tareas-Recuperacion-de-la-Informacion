{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Stiven Saldaña"
      ],
      "metadata": {
        "id": "RNs-AwBnTy9h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 6: Introducción a Dense Retrieval\n",
        "## Objetivo de la práctica\n",
        "Generar embeddings con sentence-transformers (SBERT, E5), y recuperarlos"
      ],
      "metadata": {
        "id": "s3SBFp2PT3oX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parte 0: Carga del Corpus\n",
        "## Actividad\n",
        "1. Carga el corpus 20 Newsgroups desde sklearn.datasets.fetch_20newsgroups.\n",
        "2. Limita el corpus a los primeros 2000 documentos para facilitar el procesamiento"
      ],
      "metadata": {
        "id": "wyMWbyRBT8dm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cYFv092qTDKM",
        "outputId": "8be42275-3582-4ee0-fd00-aa5bd4b1c67e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.9.0+cu126)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.11.12)\n"
          ]
        }
      ],
      "source": [
        "# se instala la libreria para usar los transformers\n",
        "!pip install sentence-transformers scikit-learn numpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# se importan librerias necesarias\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "id": "HPWguoC9UK2U"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se puede escoger entre usar el modelo SEBERT O E5\n",
        "# unicamente se descomenta y comenta la opcion que se\n",
        "# desee usar\n",
        "model_name = 'all-MiniLM-L6-v2'\n",
        "# model_name = 'intfloat/e5-base'\n",
        "model = SentenceTransformer(model_name)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "YoPyisZAUMp5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se carga el dataset excepto los encabezados\n",
        "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
        "docs_full = newsgroups.data\n",
        "# Se cargan los primeros 2000 documentos\n",
        "docs = docs_full[:2000]"
      ],
      "metadata": {
        "id": "rTJYYhKIUeKH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "c2dAv5I5Uj-D",
        "outputId": "1ddcf03c-bab7-4acf-cf2a-2cbfd879cc6b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"\\n\\nI am sure some bashers of Pens fans are pretty confused about the lack\\nof any kind of posts about the recent Pens massacre of the Devils. Actually,\\nI am  bit puzzled too and a bit relieved. However, I am going to put an end\\nto non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\\nare killing those Devils worse than I thought. Jagr just showed you why\\nhe is much better than his regular season stats. He is also a lot\\nfo fun to watch in the playoffs. Bowman should let JAgr have a lot of\\nfun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\\nregular season game.          PENS RULE!!!\\n\\n\",\n",
              " 'My brother is in the market for a high-performance video card that supports\\nVESA local bus with 1-2MB RAM.  Does anyone have suggestions/ideas on:\\n\\n  - Diamond Stealth Pro Local Bus\\n\\n  - Orchid Farenheit 1280\\n\\n  - ATI Graphics Ultra Pro\\n\\n  - Any other high-performance VLB card\\n\\n\\nPlease post or email.  Thank you!\\n\\n  - Matt\\n',\n",
              " '\\n\\n\\n\\n\\tFinally you said what you dream about. Mediterranean???? That was new....\\n\\tThe area will be \"greater\" after some years, like your \"holocaust\" numbers......\\n\\n\\n\\n\\n\\t\\t*****\\n\\tIs\\'t July in USA now????? Here in Sweden it\\'s April and still cold.\\n\\tOr have you changed your calendar???\\n\\n\\n\\t\\t\\t\\t\\t\\t    ****************\\n\\t\\t\\t\\t\\t\\t    ******************\\n\\t\\t\\t    ***************\\n\\n\\n\\tNOTHING OF THE MENTIONED IS TRUE, BUT LET SAY IT\\'s TRUE.\\n\\t\\n\\tSHALL THE AZERI WOMEN AND CHILDREN GOING TO PAY THE PRICE WITH\\n\\t\\t\\t\\t\\t\\t    **************\\n\\tBEING RAPED, KILLED AND TORTURED BY THE ARMENIANS??????????\\n\\t\\n\\tHAVE YOU HEARDED SOMETHING CALLED: \"GENEVA CONVENTION\"???????\\n\\tYOU FACIST!!!!!\\n\\n\\n\\n\\tOhhh i forgot, this is how Armenians fight, nobody has forgot\\n\\tyou killings, rapings and torture against the Kurds and Turks once\\n\\tupon a time!\\n      \\n       \\n\\n\\nOhhhh so swedish RedCross workers do lie they too? What ever you say\\n\"regional killer\", if you don\\'t like the person then shoot him that\\'s your policy.....l\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ti\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ti\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ti\\n\\tConfused?????\\t\\t\\t\\t\\t\\t\\t\\ti\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ti\\n        Search Turkish planes? You don\\'t know what you are talking about.\\ti\\n        Turkey\\'s government has announced that it\\'s giving weapons  <-----------i\\n        to Azerbadjan since Armenia started to attack Azerbadjan\\t\\t\\n        it self, not the Karabag province. So why search a plane for weapons\\t\\n        since it\\'s content is announced to be weapons?   \\n\\n\\tIf there is one that\\'s confused then that\\'s you! We have the right (and we do)\\n\\tto give weapons to the Azeris, since Armenians started the fight in Azerbadjan!\\n \\n\\n\\n\\tShoot down with what? Armenian bread and butter? Or the arms and personel \\n\\tof the Russian army?\\n\\n\\n',\n",
              " \"\\nThink!\\n\\nIt's the SCSI card doing the DMA transfers NOT the disks...\\n\\nThe SCSI card can do DMA transfers containing data from any of the SCSI devices\\nit is attached when it wants to.\\n\\nAn important feature of SCSI is the ability to detach a device. This frees the\\nSCSI bus for other devices. This is typically used in a multi-tasking OS to\\nstart transfers on several devices. While each device is seeking the data the\\nbus is free for other commands and data transfers. When the devices are\\nready to transfer the data they can aquire the bus and send the data.\\n\\nOn an IDE bus when you start a transfer the bus is busy until the disk has seeked\\nthe data and transfered it. This is typically a 10-20ms second lock out for other\\nprocesses wanting the bus irrespective of transfer time.\\n\",\n",
              " '1)    I have an old Jasmine drive which I cannot use with my new system.\\n My understanding is that I have to upsate the driver with a more modern\\none in order to gain compatability with system 7.0.1.  does anyone know\\nof an inexpensive program to do this?  ( I have seen formatters for <$20\\nbuit have no idea if they will work)\\n \\n2)     I have another ancient device, this one a tape drive for which\\nthe back utility freezes the system if I try to use it.  THe drive is a\\njasmine direct tape (bought used for $150 w/ 6 tapes, techmar\\nmechanism).  Essentially I have the same question as above, anyone know\\nof an inexpensive beckup utility I can use with system 7.0.1']"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parte 2: Generación de Embeddings\n",
        "## Actividad\n",
        "1. Usa dos modelos de sentence-transformers. Puedes usar: 'all-MiniLM-L6-v2' (SBERT), o 'intfloat/e5-base' (E5). Cuando uses E5, antepon \"passage: \" a cada documento antes de codificar.\n",
        "2. Genera los vectores de embeddings para todos los documentos usando el modelo seleccionado.\n",
        "3. Guarda los embeddings en un array de NumPy para su posterior indexación."
      ],
      "metadata": {
        "id": "ip4jH355Uzo3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# condicional en caso de esoger SEBERT o E5\n",
        "texts_to_encode = docs\n",
        "if \"e5\" in model_name:\n",
        "    texts_to_encode = [\"passage: \" + doc for doc in docs]\n",
        "\n",
        "# generacion de los vectores\n",
        "doc_embeddings = model.encode(texts_to_encode, show_progress_bar=True)\n",
        "\n",
        "# se guarda los embedings en un array  NumPy\n",
        "doc_embeddings_np = np.array(doc_embeddings)\n",
        "print(f\"--> Embeddings generados. Forma del array: {doc_embeddings_np.shape}\")"
      ],
      "metadata": {
        "id": "wWUU1rAnUvfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parte 3: Consulta\n",
        "## Actividad\n",
        "1. Escribe una consulta en lenguaje natural. Ejemplos:\n",
        "* \"God, religion, and spirituality\"\n",
        "* \"space exploration\"\n",
        "* \"car maintenance\"\n",
        "2. Codifica la consulta utilizando el mismo modelo de embeddings. Cuando uses E5, antepon \"query: \" a la consulta.\n",
        "\n",
        "3. Recupera los 5 documentos más relevantes con similitud coseno.\n",
        "\n",
        "4. Muestra los textos de los documentos recuperados (puedes mostrar solo los primeros 500 caracteres de cada uno)."
      ],
      "metadata": {
        "id": "Eguerhh7V3zf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# escritura de la consulta\n",
        "query_text = \"space exploration\"\n",
        "\n",
        "# logica que se usa en caso de usar E5\n",
        "query_input = query_text\n",
        "if \"e5\" in model_name:\n",
        "    query_input = \"query: \" + query_text\n",
        "\n",
        "# se codifica la consulta\n",
        "query_embedding = model.encode([query_input])\n",
        "\n",
        "# se calcula la similitud coseno entre la consulta y los documentos\n",
        "# se debuelve una matriz y toma la primera fila\n",
        "similarities = cosine_similarity(query_embedding, doc_embeddings_np)[0]\n",
        "\n",
        "# se toma los los 5 documentos\n",
        "top_k = 5\n",
        "top_results_indices = np.argsort(similarities)[::-1][:top_k]"
      ],
      "metadata": {
        "id": "92F2E-hqU-tf"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nResultados para: SEBERT con la consulta: '{query_text}'\\n\" + \"=\"*40)\n",
        "\n",
        "for rank, index in enumerate(top_results_indices):\n",
        "    score = similarities[index]\n",
        "    content = docs[index]\n",
        "\n",
        "    print(f\"\\nTop {rank+1} (Score: {score:.4f})\")\n",
        "    print(\"-\" * 20)\n",
        "    # Mostramos solo los primeros 500 caracteres\n",
        "    print(content[:500].strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYtfmWIBWHSm",
        "outputId": "095caf08-67e5-404b-af4c-0b2da676be5a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Resultados para: SEBERT con la consulta: 'space exploration'\n",
            "========================================\n",
            "\n",
            "Top 1 (Score: 0.4991)\n",
            "--------------------\n",
            "I am posting this for a friend without internet access. Please inquire\n",
            "to the phone number and address listed.\n",
            "---------------------------------------------------------------------\n",
            "\n",
            "\"Space: Teaching's Newest Frontier\"\n",
            "Sponsored by the Planetary Studies Foundation\n",
            "\n",
            "The Planetary Studies Foundation is sponsoring a one week class for\n",
            "teachers called \"Space: Teaching's Newest Frontier.\" The class will be\n",
            "held at the Sheraton Suites in Elk Grove, Illinois from June 14 through\n",
            "June 18. Participants wh\n",
            "\n",
            "Top 2 (Score: 0.4398)\n",
            "--------------------\n",
            "Well, here goes.\n",
            "\n",
            "The first item of business is to establish the importance space life\n",
            "sciences in the whole of scheme of humankind.  I mean compared\n",
            "to football and baseball, the average joe schmoe doesn't seem interested\n",
            "or even curious about spaceflight.  I think that this forum can\n",
            "make a major change in that lack of insight and education.\n",
            "\n",
            "All of us, in our own way, can contribute to a comprehensive document\n",
            "which can be released to the general public around the world.  The\n",
            "document would\n",
            "\n",
            "Top 3 (Score: 0.4321)\n",
            "--------------------\n",
            "Ron Miller is a space artist with a long and distinguished career.  \n",
            "I've admired both his paintings (remember the USPS Solar System\n",
            "Exploration Stamps last year?) and his writings on the history of\n",
            "spaceflight.  For several years he's been working on a *big* project\n",
            "which is almost ready to hit the streets.  A brochure from his\n",
            "publisher has landed in my mailbox, and I thought it was cool enough\n",
            "to type in part of it (it's rather long).  Especially given the Net's\n",
            "strong interest in vaporware s\n",
            "\n",
            "Top 4 (Score: 0.3995)\n",
            "--------------------\n",
            "Any comments on the absorbtion of the Office of Exploration into the\n",
            "Office of Space Sciences and the reassignment of Griffin to the \"Chief\n",
            "Engineer\" position?  Is this just a meaningless administrative\n",
            "shuffle, or does this bode ill for SEI?\n",
            "\n",
            "In my opinion, this seems like a Bad Thing, at least on the surface.\n",
            "Griffin seemed to be someone who was actually interested in getting\n",
            "things done, and who was willing to look an innovative approaches to\n",
            "getting things done faster, better, and cheaper.\n",
            "\n",
            "Top 5 (Score: 0.3746)\n",
            "--------------------\n",
            "AW&ST  had a brief blurb on a Manned Lunar Exploration confernce\n",
            "May 7th  at Crystal City Virginia, under the auspices of AIAA.\n",
            "\n",
            "Does anyone know more about this?  How much, to attend????\n",
            "\n",
            "Anyone want to go?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# USANDO E5"
      ],
      "metadata": {
        "id": "f0FReCSedO9N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Selección del modelo"
      ],
      "metadata": {
        "id": "vZB7g1vRdlb6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se puede escoger entre usar el modelo SEBERT O E5\n",
        "# unicamente se descomenta y comenta la opcion que se desee usar\n",
        "# model_name = 'all-MiniLM-L6-v2'\n",
        "model_name = 'intfloat/e5-base'\n",
        "model = SentenceTransformer(model_name)"
      ],
      "metadata": {
        "id": "oEgagYSbWIXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paso 0"
      ],
      "metadata": {
        "id": "_DDkmAXJdpsF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se carga el dataset excepto los encabezados\n",
        "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
        "docs_full = newsgroups.data\n",
        "# Se cargan los primeros 2000 documentos\n",
        "docs = docs_full[:2000]"
      ],
      "metadata": {
        "id": "FRib_uzCdYMa"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs[:5]"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlhumbTte7bG",
        "outputId": "11e095fc-7e90-404a-e943-244a5cb830e5"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"\\n\\nI am sure some bashers of Pens fans are pretty confused about the lack\\nof any kind of posts about the recent Pens massacre of the Devils. Actually,\\nI am  bit puzzled too and a bit relieved. However, I am going to put an end\\nto non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\\nare killing those Devils worse than I thought. Jagr just showed you why\\nhe is much better than his regular season stats. He is also a lot\\nfo fun to watch in the playoffs. Bowman should let JAgr have a lot of\\nfun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\\nregular season game.          PENS RULE!!!\\n\\n\",\n",
              " 'My brother is in the market for a high-performance video card that supports\\nVESA local bus with 1-2MB RAM.  Does anyone have suggestions/ideas on:\\n\\n  - Diamond Stealth Pro Local Bus\\n\\n  - Orchid Farenheit 1280\\n\\n  - ATI Graphics Ultra Pro\\n\\n  - Any other high-performance VLB card\\n\\n\\nPlease post or email.  Thank you!\\n\\n  - Matt\\n',\n",
              " '\\n\\n\\n\\n\\tFinally you said what you dream about. Mediterranean???? That was new....\\n\\tThe area will be \"greater\" after some years, like your \"holocaust\" numbers......\\n\\n\\n\\n\\n\\t\\t*****\\n\\tIs\\'t July in USA now????? Here in Sweden it\\'s April and still cold.\\n\\tOr have you changed your calendar???\\n\\n\\n\\t\\t\\t\\t\\t\\t    ****************\\n\\t\\t\\t\\t\\t\\t    ******************\\n\\t\\t\\t    ***************\\n\\n\\n\\tNOTHING OF THE MENTIONED IS TRUE, BUT LET SAY IT\\'s TRUE.\\n\\t\\n\\tSHALL THE AZERI WOMEN AND CHILDREN GOING TO PAY THE PRICE WITH\\n\\t\\t\\t\\t\\t\\t    **************\\n\\tBEING RAPED, KILLED AND TORTURED BY THE ARMENIANS??????????\\n\\t\\n\\tHAVE YOU HEARDED SOMETHING CALLED: \"GENEVA CONVENTION\"???????\\n\\tYOU FACIST!!!!!\\n\\n\\n\\n\\tOhhh i forgot, this is how Armenians fight, nobody has forgot\\n\\tyou killings, rapings and torture against the Kurds and Turks once\\n\\tupon a time!\\n      \\n       \\n\\n\\nOhhhh so swedish RedCross workers do lie they too? What ever you say\\n\"regional killer\", if you don\\'t like the person then shoot him that\\'s your policy.....l\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ti\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ti\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ti\\n\\tConfused?????\\t\\t\\t\\t\\t\\t\\t\\ti\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ti\\n        Search Turkish planes? You don\\'t know what you are talking about.\\ti\\n        Turkey\\'s government has announced that it\\'s giving weapons  <-----------i\\n        to Azerbadjan since Armenia started to attack Azerbadjan\\t\\t\\n        it self, not the Karabag province. So why search a plane for weapons\\t\\n        since it\\'s content is announced to be weapons?   \\n\\n\\tIf there is one that\\'s confused then that\\'s you! We have the right (and we do)\\n\\tto give weapons to the Azeris, since Armenians started the fight in Azerbadjan!\\n \\n\\n\\n\\tShoot down with what? Armenian bread and butter? Or the arms and personel \\n\\tof the Russian army?\\n\\n\\n',\n",
              " \"\\nThink!\\n\\nIt's the SCSI card doing the DMA transfers NOT the disks...\\n\\nThe SCSI card can do DMA transfers containing data from any of the SCSI devices\\nit is attached when it wants to.\\n\\nAn important feature of SCSI is the ability to detach a device. This frees the\\nSCSI bus for other devices. This is typically used in a multi-tasking OS to\\nstart transfers on several devices. While each device is seeking the data the\\nbus is free for other commands and data transfers. When the devices are\\nready to transfer the data they can aquire the bus and send the data.\\n\\nOn an IDE bus when you start a transfer the bus is busy until the disk has seeked\\nthe data and transfered it. This is typically a 10-20ms second lock out for other\\nprocesses wanting the bus irrespective of transfer time.\\n\",\n",
              " '1)    I have an old Jasmine drive which I cannot use with my new system.\\n My understanding is that I have to upsate the driver with a more modern\\none in order to gain compatability with system 7.0.1.  does anyone know\\nof an inexpensive program to do this?  ( I have seen formatters for <$20\\nbuit have no idea if they will work)\\n \\n2)     I have another ancient device, this one a tape drive for which\\nthe back utility freezes the system if I try to use it.  THe drive is a\\njasmine direct tape (bought used for $150 w/ 6 tapes, techmar\\nmechanism).  Essentially I have the same question as above, anyone know\\nof an inexpensive beckup utility I can use with system 7.0.1']"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paso 2"
      ],
      "metadata": {
        "id": "R_qP3HFtdt7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# condicional en caso de esoger SEBERT o E5\n",
        "texts_to_encode = docs\n",
        "if \"e5\" in model_name:\n",
        "    texts_to_encode = [\"passage: \" + doc for doc in docs]\n",
        "\n",
        "# generacion de los vectores\n",
        "doc_embeddings = model.encode(texts_to_encode, show_progress_bar=True)\n",
        "\n",
        "# se guarda los embedings en un array  NumPy\n",
        "doc_embeddings_np = np.array(doc_embeddings)\n",
        "print(f\"--> Embeddings generados. Forma del array: {doc_embeddings_np.shape}\")"
      ],
      "metadata": {
        "id": "yn6XfOsFdYy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paso 3"
      ],
      "metadata": {
        "id": "R-cAAXpKdvoH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# escritura de la consulta\n",
        "query_text2 = \"space exploration\"\n",
        "\n",
        "# logica que se usa en caso de usar E5\n",
        "query_input2 = query_text2\n",
        "if \"e5\" in model_name:\n",
        "    query_input2 = \"query: \" + query_text2\n",
        "\n",
        "# se codifica la consulta\n",
        "query_embedding = model.encode([query_input2])\n",
        "\n",
        "# se calcula la similitud coseno entre la consulta y los documentos\n",
        "similarities = cosine_similarity(query_embedding, doc_embeddings_np)[0]\n",
        "\n",
        "# se toma los 5 documentos\n",
        "top_k = 5\n",
        "top_results_indices = np.argsort(similarities)[::-1][:top_k]"
      ],
      "metadata": {
        "id": "JEPvh4GLdfpy"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reaultados"
      ],
      "metadata": {
        "id": "je-JEEOVdy4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nResultados para E5: '{query_text2}'\\n\" + \"=\"*40)\n",
        "\n",
        "for rank, index in enumerate(top_results_indices):\n",
        "    score2 = similarities[index]\n",
        "    content2 = docs[index]\n",
        "\n",
        "    print(f\"\\nTop {rank+1} (Score: {score2:.4f})\")\n",
        "    print(\"-\" * 20)\n",
        "    print(content2[:500].strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yd7XkkgGdjNW",
        "outputId": "9d27b099-3ca4-4b92-dd6b-c6ffaa1b0ec8"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Resultados para E5: 'space exploration'\n",
            "========================================\n",
            "\n",
            "Top 1 (Score: 0.8190)\n",
            "--------------------\n",
            "AW&ST  had a brief blurb on a Manned Lunar Exploration confernce\n",
            "May 7th  at Crystal City Virginia, under the auspices of AIAA.\n",
            "\n",
            "Does anyone know more about this?  How much, to attend????\n",
            "\n",
            "Anyone want to go?\n",
            "\n",
            "Top 2 (Score: 0.8179)\n",
            "--------------------\n",
            "Well, here goes.\n",
            "\n",
            "The first item of business is to establish the importance space life\n",
            "sciences in the whole of scheme of humankind.  I mean compared\n",
            "to football and baseball, the average joe schmoe doesn't seem interested\n",
            "or even curious about spaceflight.  I think that this forum can\n",
            "make a major change in that lack of insight and education.\n",
            "\n",
            "All of us, in our own way, can contribute to a comprehensive document\n",
            "which can be released to the general public around the world.  The\n",
            "document would\n",
            "\n",
            "Top 3 (Score: 0.8150)\n",
            "--------------------\n",
            "I am posting this for a friend without internet access. Please inquire\n",
            "to the phone number and address listed.\n",
            "---------------------------------------------------------------------\n",
            "\n",
            "\"Space: Teaching's Newest Frontier\"\n",
            "Sponsored by the Planetary Studies Foundation\n",
            "\n",
            "The Planetary Studies Foundation is sponsoring a one week class for\n",
            "teachers called \"Space: Teaching's Newest Frontier.\" The class will be\n",
            "held at the Sheraton Suites in Elk Grove, Illinois from June 14 through\n",
            "June 18. Participants wh\n",
            "\n",
            "Top 4 (Score: 0.8094)\n",
            "--------------------\n",
            "As for SF and advertising in space. There is a romantic episode\n",
            "in Mead's \"The Big Ball of Wax\" where the lovers are watching \n",
            "the constellation Pepsi Cola rising over the horizon and noting\n",
            "the some 'stars' had slipped cause the Teamsters were on strike.\n",
            "\n",
            "This was the inspiration for my article on orbiting a formation\n",
            "of space mirrors published in Spaceflight in 1986. As the reviews\n",
            "but is it aesthetically desirable?  These days the only aesthetics\n",
            "that count are the ones you can count!\n",
            "\n",
            "Top 5 (Score: 0.8084)\n",
            "--------------------\n",
            "Whatabout, Schools, Universities, Rich Individuals (around 250 people \n",
            "in the UK have more than 10 million dollars each). I reecieved mail\n",
            "from people who claimed they might get a person into space for $500\n",
            "per pound. Send a skinny person into space and split the rest of the money\n",
            "among the ground crew!\n",
            "Agreed. I volunteer for any UK attempts. But one clause: No launch methods\n",
            "which are clearly dangerous to the environment (ours or someone else's). No\n",
            "usage of materials from areas of planetary\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qeXyZowtsbsJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}